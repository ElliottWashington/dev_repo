{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import datetime\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(date, target, sender):    \n",
    "    # Establish a connection to the PostgreSQL database\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"10.7.8.59\",\n",
    "        database=\"fixtransactions\",\n",
    "        user=\"scalp\",\n",
    "        password=\"QAtr@de442\",\n",
    "        port=\"5433\"\n",
    "    )\n",
    "    \n",
    "    # Define the SQL queries\n",
    "    query1 = f\"\"\"\n",
    "    CREATE TEMP TABLE temp_table1 AS\n",
    "    SELECT tag11, tag100, tag41, tag49 \n",
    "    FROM fixmsg \n",
    "    WHERE tag52 >= TIMESTAMP '{date}'\n",
    "    AND tag52 < TIMESTAMP '{date}' + INTERVAL '1 DAY'\n",
    "    AND tag56 = '{target}'\n",
    "    and tag49 = '{sender}'\n",
    "    AND (tag35 = 'AB' or tag35 = 'D');\n",
    "    \"\"\"\n",
    "\n",
    "    query2 = f\"\"\"\n",
    "    CREATE TEMP TABLE temp_table2 AS\n",
    "    SELECT a.tag11, a.tag52 AS cancel_52, b.tag52 AS cancelack_52, b.tag41, a.tag100\n",
    "    FROM fixmsg a\n",
    "    JOIN fixmsg b ON a.tag11 = b.tag41\n",
    "    WHERE a.tag52 >= TIMESTAMP '{date}'\n",
    "    AND a.tag52 < TIMESTAMP '{date}' + INTERVAL '1 DAY'\n",
    "    AND b.tag52 >= TIMESTAMP '{date}'\n",
    "    AND b.tag52 < TIMESTAMP '{date}' + INTERVAL '1 DAY'\n",
    "    and a.tag49 = '{sender}'\n",
    "    and b.tag56 = '{sender}'\n",
    "    and b.tag49 = '{target}'\n",
    "    AND b.tag35 = '8'\n",
    "    AND b.tag39 = '4'\n",
    "    AND (a.tag35 = 'AB' or a.tag35 = 'D')\n",
    "    AND a.tag56 = '{target}';\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query1)\n",
    "        cursor.execute(query2)\n",
    "\n",
    " # Load the query results into dataframes\n",
    "    table2_df = pd.read_sql_query(\"SELECT * FROM temp_table2\", conn)\n",
    "\n",
    "    # Write dataframes to CSV files with date in the file names\n",
    "    table2_df.to_csv(f\"/home/elliott/development/files/equity_nack_latencies_{date}.csv\", index=False)\n",
    "    \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "    return table2_df\n",
    "\n",
    "def write_csv(date, target):\n",
    "    # Load the query results from CSV files\n",
    "    table2_df = pd.read_csv(f\"/home/elliott/development/files/equity_nack_latencies_{date}.csv\")\n",
    "    \n",
    "    # Calculate time differences and filter the data\n",
    "    table2_df[\"cancel_52\"] = pd.to_datetime(table2_df[\"cancel_52\"])\n",
    "    table2_df[\"cancelack_52\"] = pd.to_datetime(table2_df[\"cancelack_52\"])\n",
    "    table2_df[\"time_diff\"] = (table2_df[\"cancelack_52\"] - table2_df[\"cancel_52\"]).apply(lambda x: x.total_seconds())\n",
    "    table2_df = table2_df.sort_values(by=\"time_diff\", ascending=False)\n",
    "    table2_df_filtered = table2_df[table2_df[\"time_diff\"] >= 1]\n",
    "    \n",
    "    # Compute the threshold and filter the data again\n",
    "    threshold = table2_df_filtered[\"time_diff\"].quantile(0.98)\n",
    "    top_2_percent = table2_df_filtered[table2_df_filtered[\"time_diff\"] >= threshold]\n",
    "    \n",
    "    # Compute the counts and merge the dataframes\n",
    "    counts = top_2_percent.groupby(\"tag100\").size().reset_index(name=\"counts\")\n",
    "    result_df = pd.merge(top_2_percent, counts, on=\"tag100\", how=\"left\")\n",
    "    \n",
    "    # Write the final dataframe to a CSV file\n",
    "    result_df.to_csv(f\"/home/elliott/development/files/equity_cancelack_{target}_2%highest_latencies_{date}.csv\", index=False, )\n",
    "\n",
    "def run_analysis(date, target):\n",
    "    # Read a CSV file located at '/home/elliott/development/files/options_nack_latencies_2023-03-10.csv' and store it in a variable called 'table2_df'\n",
    "    table2_df = pd.read_csv(f'/home/elliott/development/files/equity_nack_latencies_{date}.csv')\n",
    "\n",
    "    # Convert the 'cancel_52' column in 'table2_df' to datetime format using the 'pd.to_datetime()' function\n",
    "    table2_df['cancel_52'] = pd.to_datetime(table2_df['cancel_52'])\n",
    "\n",
    "    # Convert the 'cancelack_52' column in 'table2_df' to datetime format using the 'pd.to_datetime()' function\n",
    "    table2_df['cancelack_52'] = pd.to_datetime(table2_df['cancelack_52'])\n",
    "\n",
    "    # Calculate the time difference in seconds between 'cancel_52' and 'cancelack_52' columns in 'table2_df' and store it in a new column called 'time_diff'\n",
    "    table2_df[\"time_diff\"] = (table2_df[\"cancelack_52\"] - table2_df[\"cancel_52\"]).apply(lambda x: x.total_seconds())\n",
    "\n",
    "    # Sort 'table2_df' in descending order based on 'time_diff'\n",
    "    table2_df = table2_df.sort_values(by='time_diff', ascending=False)\n",
    "\n",
    "    # Create a new DataFrame called 'table2_df_filtered' by filtering out rows where 'time_diff' equals 0 in 'table2_df'\n",
    "    table2_df_filtered = table2_df[table2_df['time_diff'] != 0]\n",
    "\n",
    "    # Calculate the 90th percentile of 'timeentile of 'time_diff_diffiff' in 'table2_df_filtered' and store it in a variable called 'threshold'\n",
    "    threshold = table2_df_filtered['time_diff'].quantile(0.90)\n",
    "\n",
    "    # Create a new DataFrame called 'top_2_percent' by filtering out rows in 'table2_df_filtered' where 'time_diff' is less than 'threshold'\n",
    "    top_2_percent = table2_df_filtered[table2_df_filtered['time_diff'] >= threshold]\n",
    "\n",
    "    # Count the occurrences of each value in the 'tag100' column in 'top_2_percent' and store the counts in a Series called 'counts'\n",
    "    counts = top_2_percent.groupby('tag100')['tag100'].value_counts()\n",
    "\n",
    "    # Drop the duplicate 'tag100' index from the 'counts' Series and store the result in a new DataFrame called 'single_df'\n",
    "    single_df = counts.reset_index(level=1, drop=True)\n",
    "    single_df = single_df.to_frame('counts')\n",
    "    single_df = single_df.reset_index()\n",
    "\n",
    "    # Merge 'single_df' into 'top_2_percent' based on the 'tag100' column and store the result in a new DataFrame called 'result_df'\n",
    "    result_df = top_2_percent.merge(single_df, on='tag100', how='left')\n",
    "\n",
    "    result_df.to_csv(f\"/home/elliott/development/files/equity_cancelack_{target}_2%highest_latencies_{date}.csv\", index=False)\n",
    "\n",
    "def main():\n",
    "    # Get command line arguments\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: python script.py <date> <target> <sender>\")\n",
    "        sys.exit(1)\n",
    "    date = sys.argv[1]\n",
    "    target = sys.argv[2]\n",
    "    sender = sys.argv[3]\n",
    "\n",
    "    # Call the 'run_queries' function with the command line arguments\n",
    "    run_queries(date, target, sender)\n",
    "    run_analysis(date, target)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
