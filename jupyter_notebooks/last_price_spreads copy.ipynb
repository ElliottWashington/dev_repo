{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1: 1.1686136450152844\n",
      "time2: 11.002577741979621\n",
      "time3: 37.640348635963164\n",
      "Time taken for get_batch_data1: 63.76835818600375 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 544\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    543\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m--> 544\u001b[0m     run()\n\u001b[1;32m    545\u001b[0m     end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    546\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal runtime: \u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 465\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m# Measure time for assign_closest_prices\u001b[39;00m\n\u001b[1;32m    464\u001b[0m start_time_assign_closest \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m--> 465\u001b[0m merged_prices_df \u001b[39m=\u001b[39m assign_closest_prices_parallel(merged_df, pricesattime_df)\n\u001b[1;32m    466\u001b[0m merged_prices_df[\u001b[39m'\u001b[39m\u001b[39mequity_change_pct\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ((merged_prices_df[\u001b[39m'\u001b[39m\u001b[39mprc_eqt_at_time\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m-\u001b[39m merged_prices_df[\u001b[39m'\u001b[39m\u001b[39mprice_at_915\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m merged_prices_df[\u001b[39m'\u001b[39m\u001b[39mprice_at_915\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    467\u001b[0m end_time_assign_closest \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "Cell \u001b[0;32mIn[24], line 391\u001b[0m, in \u001b[0;36massign_closest_prices_parallel\u001b[0;34m(original_df, batch_data_df)\u001b[0m\n\u001b[1;32m    389\u001b[0m unique_symbols \u001b[39m=\u001b[39m original_df[\u001b[39m'\u001b[39m\u001b[39munderly\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor() \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 391\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(executor\u001b[39m.\u001b[39;49mmap(assign_closest_prices, \n\u001b[1;32m    392\u001b[0m                                 [original_df[original_df[\u001b[39m'\u001b[39;49m\u001b[39munderly\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m symbol] \u001b[39mfor\u001b[39;49;00m symbol \u001b[39min\u001b[39;49;00m unique_symbols],\n\u001b[1;32m    393\u001b[0m                                 [batch_data_df[batch_data_df[\u001b[39m'\u001b[39;49m\u001b[39mSymbol\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m symbol] \u001b[39mfor\u001b[39;49;00m symbol \u001b[39min\u001b[39;49;00m unique_symbols]))\n\u001b[1;32m    394\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat(results, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sqlalchemy import create_engine\n",
    "import zipfile\n",
    "\n",
    "connection = psycopg2.connect(host=\"10.5.1.20\", database=\"marketdata\", user=\"elliott\", password=\"scalp\", port='5432')\n",
    "connection2 = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "\n",
    "m_root = 'RBadmin'\n",
    "m_password = '$Calp123'\n",
    "m_host = '10.5.1.32'\n",
    "m_db = 'rbandits2'\n",
    "uri = f\"mysql+mysqldb://{m_root}:{m_password}@{m_host}/{m_db}\"\n",
    "mydb = create_engine(uri, connect_args={'ssl_mode': 'DISABLED'})\n",
    "mapping = {0: \"Vertical\",5: \"Straddle\", 6: \"Strangle\"}\n",
    "\n",
    "def identify_inverted(row):\n",
    "    # Ignore the rows where either 'BBOAsk' or 'BBOBid' is zero\n",
    "    if row['BBOAsk'] == 0 or row['BBOBid'] == 0:\n",
    "        return ''\n",
    "    # Check if 'BBOAsk' is less than 'BBOBid'\n",
    "    elif row['BBOAsk'] < row['BBOBid']:\n",
    "        return 'Inverted'\n",
    "    else:\n",
    "        return ''\n",
    "def identify_option_type(row):\n",
    "    # Check if the 'sprdtype' is 'Vertical'\n",
    "    if row['sprdtype'] == 'Vertical':\n",
    "        # Check for 'C' or 'P' surrounded by digits in 'formatted_symbol'\n",
    "        matches = re.findall(r'\\d(C|P)\\d', row['formatted_symbol'])\n",
    "        if matches:\n",
    "            # Take the first match and check the middle character\n",
    "            if 'C' in matches[0]:\n",
    "                return 'Call Vertical'\n",
    "            else:\n",
    "                \n",
    "                return 'Put Vertical'\n",
    "    # If 'sprdtype' is not 'Vertical', return the original 'sprdtype'\n",
    "    return row['sprdtype']\n",
    "def get_data(date):\n",
    "    query = f\"\"\"\n",
    "                SELECT ts as Time, underly, sprdsym as Spread, price as LastPrice, sprdtype\n",
    "                FROM trdsprd\n",
    "                WHERE ts >= DATE_SUB('{date}', INTERVAL 1 DAY)\n",
    "                AND DAYOFWEEK(ts) BETWEEN 2 AND 6\n",
    "                AND sprdtype in ('0', '5', '6')\n",
    "                AND underly NOT LIKE 'QQQ%%'\n",
    "                AND underly NOT LIKE 'SPY%%'\n",
    "                AND underly NOT LIKE 'IWM%%'\n",
    "                ORDER BY ts DESC\n",
    "            \"\"\"\n",
    "    df = pd.read_sql_query(query, mydb)\n",
    "    return df\n",
    "def convert_to_new_format(option):\n",
    "    if pd.isnull(option):\n",
    "        return ''  # Return an empty string for NaN values\n",
    "\n",
    "    parts = option.split('_')  # Split the spread into individual options\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        # Identify the beginning of the date substring\n",
    "        date_start = None\n",
    "        for i in range(len(part) - 5):  # Subtract 5 to avoid running off the end of the string\n",
    "            if part[i:i+2].isdigit() and part[i+2:i+4].isdigit() and part[i+4:i+6].isdigit():\n",
    "                date_start = i\n",
    "                break\n",
    "        # If a date substring couldn't be found, treat the part as a non-option\n",
    "        if date_start is None:\n",
    "            new_parts.append(part)\n",
    "            continue\n",
    "        # Extract underlying, date, call/put, strike, and suffix\n",
    "        underlying = part[:date_start]\n",
    "        date = '20' + part[date_start:date_start+6]  # Convert YY to YYYY\n",
    "        cp = part[date_start+6]\n",
    "        strike_start = date_start + 7\n",
    "        strike_end = strike_start\n",
    "        for char in part[strike_start:]:\n",
    "            if char.isdigit() or char == '.':\n",
    "                strike_end += 1\n",
    "            else:\n",
    "                break\n",
    "        strike = part[strike_start:strike_end]\n",
    "        # Append decimal and trailing zeros if necessary\n",
    "        if '.' not in strike:\n",
    "            strike += '.00'\n",
    "        elif len(strike.split('.')[1]) == 1:\n",
    "            strike += '0'\n",
    "        suffix = part[strike_end:]\n",
    "        new_part = underlying + date + cp + strike + suffix\n",
    "        new_parts.append(new_part)\n",
    "    return '_'.join(new_parts)  # Join the options back into a spread\n",
    "def get_batch_data2(df, current_date):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    symbols = df['underly'].unique()\n",
    "    \n",
    "    # Convert the current date string to a datetime.date object\n",
    "    current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "    \n",
    "    # Calculate the target times\n",
    "    target_time_915 = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "    target_time_914 = datetime.combine(current_date_obj, pd.Timestamp(\"9:14:00\").time())\n",
    "    \n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    (tradets BETWEEN %s AND %s)\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, target_time_914, target_time_915))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def get_batch_data1(df):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    min_time = df['Time'].min()\n",
    "    max_time = df['Time'].max()\n",
    "    symbols = df['underly'].unique()\n",
    "\n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    tradets BETWEEN %s AND %s\n",
    "            \"\"\"\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, min_time, max_time))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def assign_closest_prices(original_df, batch_data_df):\n",
    "    # Create a new column for prc_eqt_at_time\n",
    "    original_df = original_df.copy() \n",
    "    original_df['prc_eqt_at_time'] = None\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in original_df['underly'].unique():\n",
    "        # Work on a copy of the slice\n",
    "        original_subset = original_df[original_df['underly'] == symbol].copy()\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "        \n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "\n",
    "        # For each row in the original subset, find the closest timestamp in the batch subset\n",
    "        for idx, row in original_subset.iterrows():\n",
    "            time_diffs = (batch_subset['Timestamp'] - row['Time']).abs()\n",
    "            \n",
    "            # If there are no time differences, skip this row\n",
    "            if time_diffs.empty:\n",
    "                continue\n",
    "\n",
    "            closest_idx = time_diffs.idxmin()\n",
    "            closest_trade_price = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "\n",
    "            # Assign the closest trade price to the prc_eqt_at_time column\n",
    "            original_subset.at[idx, 'prc_eqt_at_time'] = closest_trade_price\n",
    "\n",
    "        # Update the original DataFrame\n",
    "        original_df.update(original_subset)\n",
    "\n",
    "    return original_df\n",
    "def send_email(names, subject, attachment_path):\n",
    "    fromaddr = \"reports@scalptrade.com\"\n",
    "    toaddr = names\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = fromaddr\n",
    "    msg['To'] = \", \".join(toaddr)\n",
    "    msg['Subject'] = '{}'.format(subject)\n",
    "\n",
    "    # Attach the CSV file\n",
    "    attachment = open(attachment_path, 'rb')\n",
    "    part = MIMEBase('application', 'octet-stream')\n",
    "    part.set_payload(attachment.read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(attachment_path)}\"')\n",
    "    msg.attach(part)\n",
    "\n",
    "    s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    s.starttls()\n",
    "    s.login(fromaddr, \"sc@lptrade\")\n",
    "    text = msg.as_string()\n",
    "    s.sendmail(fromaddr, toaddr, text)\n",
    "def round_to_two_decimals(df, columns):\n",
    "    \"\"\"Format specified columns in a dataframe to two decimal places as strings.\"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: \"{:.2f}\".format(float(x)) if pd.notna(x) else None)\n",
    "    return df\n",
    "def assign_equity_prices_at_915(original_df, batch_data_df, current_date):\n",
    "    # Create a new column for price at 915\n",
    "    original_df['price_at_915'] = None\n",
    "    \n",
    "    # Get a unique set of symbols\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in unique_symbols:\n",
    "        # Filter batch data for the current symbol\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "\n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "        \n",
    "        # Convert the current_date string to a datetime.date object\n",
    "        current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "        \n",
    "        # Calculate the target time\n",
    "        target_time = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "        \n",
    "        time_diffs = batch_subset['Timestamp'].apply(lambda x: abs(datetime.combine(current_date_obj, x.time()) - target_time))\n",
    "        closest_idx = time_diffs.idxmin()\n",
    "        price_at_915 = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "                \n",
    "        # Assign the price to all rows with the matching symbol\n",
    "        original_df.loc[original_df['underly'] == symbol, 'price_at_915'] = price_at_915\n",
    "    return original_df\n",
    "def generate_and_execute_sql(result_df):\n",
    "    new_df = result_df[['Symbol', 'underly']].copy()\n",
    "    new_df['Symbol'] = new_df['Symbol'].str.split('_')\n",
    "    exploded_df = new_df.explode('Symbol', ignore_index=True)\n",
    "    unique_underly_values = exploded_df['underly'].unique()\n",
    "\n",
    "    unique_underly_str = ','.join(f\"'{symbol}%'\" for symbol in unique_underly_values)\n",
    "    unique_underly_str = f\"ARRAY[{unique_underly_str}]\"\n",
    "\n",
    "    # Database connection\n",
    "    connection = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT theo_rate, brents_vol, s.symbol, underlying\n",
    "        FROM public.theo_rates r, public.theo_opt_sym_chain s\n",
    "        WHERE r.theo_symbol_id = s.theo_symbol_id\n",
    "        AND symbol LIKE ANY({unique_underly_str})\n",
    "        AND r.create_time > timestamp '2023-08-28 16:00:00'\n",
    "        ORDER BY strike, r.create_time DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    query_results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    df = pd.DataFrame(query_results, columns=['theo_rate', 'volatility', 'symbol', 'underlying'])\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        symbol = row['symbol']\n",
    "        theo_rate = row['theo_rate']\n",
    "        volatility = row['volatility']\n",
    "    \n",
    "        result_dict[symbol] = {'theo_rate': theo_rate, 'volatility': volatility}\n",
    "\n",
    "    return result_dict\n",
    "def lookup_rates_and_vols(row, rates_vol_dict):\n",
    "    symbols = row['Symbol'].split('_')\n",
    "    modified_symbols = [symbol[:-2] for symbol in symbols]\n",
    "    \n",
    "    if len(modified_symbols) != 2:\n",
    "        #print(f\"Skipping row with symbols: {modified_symbols}\")\n",
    "        return row  # or set specific columns to NaN or some default value\n",
    "    \n",
    "    try:\n",
    "        symbol1, symbol2 = modified_symbols\n",
    "        row['rate1'] = rates_vol_dict[symbol1]['theo_rate']\n",
    "        row['rate2'] = rates_vol_dict[symbol2]['theo_rate']\n",
    "        row['vol1'] = rates_vol_dict[symbol1]['volatility']\n",
    "        row['vol2'] = rates_vol_dict[symbol2]['volatility']\n",
    "    except KeyError:\n",
    "        #print(f\"Symbols {modified_symbols} not found in rates_vol_dict.\")\n",
    "        row['rate1'], row['rate2'], row['vol1'], row['vol2'] = [np.nan]*4\n",
    "    \n",
    "    return row\n",
    "def black_scholes(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    d2 = d1 - sigma * np.sqrt(t)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        option_price = S * np.exp(-q * t) * stats.norm.cdf(d1) - X * np.exp(-r * t) * stats.norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        option_price = X * np.exp(-r * t) * stats.norm.cdf(-d2) - S * np.exp(-q * t) * stats.norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "        \n",
    "    return option_price\n",
    "def black_scholes_delta(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        delta = np.exp(-q * t) * stats.norm.cdf(d1)\n",
    "    elif option_type == 'put':\n",
    "        delta = np.exp(-q * t) * (stats.norm.cdf(d1) - 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "    \n",
    "    return delta\n",
    "def calculate_black_scholes_and_delta(row):\n",
    "    S = float(row['prc_eqt_at_time'])  # Convert to float\n",
    "    X1 = float(row['Symbol'].split('_')[0].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    X2 = float(row['Symbol'].split('_')[1].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    t = 30 / 365  # Assuming 30 days to expiration (replace with actual value)\n",
    "    r1 = row['rate1']\n",
    "    r2 = row['rate2']\n",
    "    q = 0  # Assuming no dividend yield\n",
    "    sigma1 = row['vol1']\n",
    "    sigma2 = row['vol2']\n",
    "    \n",
    "    bs_price1 = black_scholes(S, X1, t, r1, q, sigma1)\n",
    "    bs_price2 = black_scholes(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    delta1 = black_scholes_delta(S, X1, t, r1, q, sigma1)\n",
    "    delta2 = black_scholes_delta(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    row['BS_Price1'] = bs_price1\n",
    "    row['BS_Price2'] = bs_price2\n",
    "    row['Delta1'] = delta1\n",
    "    row['Delta2'] = delta2\n",
    "    row['Spread_Delta'] = delta1 - delta2\n",
    "    \n",
    "    return row\n",
    "def process_put_verticals(row):\n",
    "    if row['sprdtype'] == 'Put Vertical':\n",
    "        # Swap BBOAsk and BBOBid\n",
    "        row['BBOAsk'], row['BBOBid'] = row['BBOBid'], row['BBOAsk']\n",
    "        \n",
    "        # Swap BBOAskSize and BBOBidSize (assuming you have these columns)\n",
    "        row['BBOAskSize'], row['BBOBidSize'] = row['BBOBidSize'], row['BBOAskSize']\n",
    "\n",
    "        # Take absolute values of LastPrice, BBOBid, and BBOAsk\n",
    "        row['LastPrice'] = abs(row['LastPrice'])\n",
    "        row['BBOBid'] = abs(row['BBOBid'])\n",
    "        row['BBOAsk'] = abs(row['BBOAsk'])\n",
    "        \n",
    "    return row\n",
    "def get_data_parallel(dates, io_threads):\n",
    "    with ThreadPoolExecutor(max_workers=io_threads) as executor:\n",
    "        results = list(executor.map(get_data, dates))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def get_batch_data1_parallel(df, io_threads):\n",
    "    unique_symbols = df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=io_threads) as executor:\n",
    "        results = list(executor.map(get_batch_data1, [df[df['underly'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def assign_closest_prices_parallel(original_df, batch_data_df, io_threads):\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=io_threads) as executor:\n",
    "        results = list(executor.map(assign_closest_prices, \n",
    "                                    [original_df[original_df['underly'] == symbol] for symbol in unique_symbols],\n",
    "                                    [batch_data_df[batch_data_df['Symbol'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def generate_and_execute_sql_parallel(result_df, cpu_threads):\n",
    "    unique_underly_values = result_df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=cpu_threads) as executor:\n",
    "        results = list(executor.map(generate_and_execute_sql, [result_df[result_df['underly'] == symbol] for symbol in unique_underly_values]))\n",
    "    return {k: v for d in results for k, v in d.items()}\n",
    "def set1(df):\n",
    "    condition1 = (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    condition2 = (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    return df[condition1 | condition2]\n",
    "def set2(df):\n",
    "    condition = (\n",
    "        df['equity_change_pct'].abs() > 1\n",
    "    ) & ((df['Spread_Delta'].abs() > .25))\n",
    "    return df[condition]\n",
    "def run():\n",
    "    start_time_1 = time.perf_counter()\n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    main_df = pd.read_csv(f'/home/elliott/Development/scripts/jupyter_notebooks/spreads-{date}0915.csv')\n",
    "    main_df = main_df[['Symbol', 'BBOAsk', 'BBOBid', 'BBOBidSize', 'BBOAskSize']].copy()\n",
    "    main_df = main_df[(main_df['BBOBid'] != 0) | (main_df['BBOAsk'] != 0)].copy()\n",
    "    main_df['formatted_symbol'] = main_df['Symbol'].apply(convert_to_new_format)\n",
    "    end_time_1 = time.perf_counter()\n",
    "    print(f\"time1: {end_time_1 - start_time_1}\")\n",
    "\n",
    "    start_time_2 = time.perf_counter()\n",
    "    db_df = get_data_parallel([date])\n",
    "    merged_df = main_df.merge(db_df, how='left', left_on='formatted_symbol', right_on='Spread').copy()\n",
    "    merged_df.dropna(inplace=True)\n",
    "    merged_df['sprdtype'] = merged_df['sprdtype'].map(mapping)\n",
    "    merged_df['sprdtype'] = merged_df.apply(identify_option_type, axis=1)\n",
    "    merged_df['Inverted'] = merged_df.apply(identify_inverted, axis=1)\n",
    "    merged_df = merged_df.apply(process_put_verticals, axis=1)\n",
    "    merged_df.sort_values(by='Time', ascending=False, inplace=True)\n",
    "    merged_df.drop_duplicates(subset='formatted_symbol', keep='first', inplace=True)\n",
    "    end_time_2 = time.perf_counter()\n",
    "    print(f\"time2: {end_time_2 - start_time_2}\")\n",
    "\n",
    "    start_time_3 = time.perf_counter()\n",
    "    prices915_df = get_batch_data2(merged_df,date)\n",
    "    assign_equity_prices_at_915(merged_df, prices915_df, date)  # This line was added\n",
    "    end_time_3 = time.perf_counter()\n",
    "    print(f\"time3: {end_time_3 - start_time_3}\")\n",
    "\n",
    "    # Measure time for get_batch_data1\n",
    "    start_time_get_batch = time.perf_counter()\n",
    "    pricesattime_df = get_batch_data1_parallel(merged_df)\n",
    "    end_time_get_batch = time.perf_counter()\n",
    "    print(f\"Time taken for get_batch_data1: {end_time_get_batch - start_time_get_batch} seconds\")\n",
    "\n",
    "    # Measure time for assign_closest_prices\n",
    "    start_time_assign_closest = time.perf_counter()\n",
    "    merged_prices_df = assign_closest_prices_parallel(merged_df, pricesattime_df)\n",
    "    merged_prices_df['equity_change_pct'] = ((merged_prices_df['prc_eqt_at_time'] - merged_prices_df['price_at_915']) / merged_prices_df['price_at_915']) * 100\n",
    "    end_time_assign_closest = time.perf_counter()\n",
    "    print(f\"Time taken for assign_closest_prices: {end_time_assign_closest - start_time_assign_closest} seconds\")\n",
    "\n",
    "\n",
    "    start_time_5 = time.perf_counter()\n",
    "    column_order = ['formatted_symbol', 'Symbol', 'Inverted', 'sprdtype', 'LastPrice', 'BBOBid', 'BBOBidSize',\n",
    "                    'BBOAsk', 'BBOAskSize', 'Time', 'underly', 'prc_eqt_at_time', 'price_at_915']\n",
    "    result_df = (merged_prices_df.loc[:, column_order]\n",
    "                .dropna(subset=['price_at_915', 'prc_eqt_at_time'])\n",
    "                .assign(PriceDelta=lambda df: df['price_at_915'].astype(float) - df['prc_eqt_at_time'],\n",
    "                        BidEdge=lambda df: np.where(df['BBOBid'] != 0, df['BBOBid'] - df['LastPrice'], np.nan),\n",
    "                        OfferEdge=lambda df: np.where(df['BBOAsk'] != 0, df['LastPrice'] - df['BBOAsk'], np.nan))\n",
    "            ).copy()\n",
    "    result_df = round_to_two_decimals(result_df, columns=['BidEdge', 'OfferEdge', 'prc_eqt_at_time', 'price_at_915', \n",
    "                                                        'PriceDelta', 'LastPrice', 'BBOBid', 'BBOAsk'])\n",
    "    result_df['BidEdge'] = result_df['BidEdge'].astype(float)\n",
    "    result_df['OfferEdge'] = result_df['OfferEdge'].astype(float)\n",
    "    result_df = result_df.loc[(result_df['BidEdge'] < -0.20) | (result_df['OfferEdge'] < -0.20)]\n",
    "    end_time_5 = time.perf_counter()\n",
    "    print(f\"time5: {end_time_5 - start_time_5}\")\n",
    "\n",
    "    start_time_6 = time.perf_counter()\n",
    "    vol_rates_dict = generate_and_execute_sql_parallel(result_df)\n",
    "    result_df = result_df.apply(lookup_rates_and_vols, axis=1, args=(vol_rates_dict,))\n",
    "    result_df.dropna(subset=['rate1'], inplace=True)\n",
    "    result_df = result_df.apply(calculate_black_scholes_and_delta, axis=1)\n",
    "    end_time_6 = time.perf_counter()\n",
    "    print(f\"time6: {end_time_6 - start_time_6}\")\n",
    "    \n",
    "\n",
    "    start_time_7 = time.perf_counter()\n",
    "    result_df.drop(columns=[\"Symbol\"], inplace=True)\n",
    "    result_df['PriceDelta'] = pd.to_numeric(result_df['PriceDelta'], errors='coerce')\n",
    "    result_df['OfferEdge'] = pd.to_numeric(result_df['OfferEdge'], errors='coerce')\n",
    "    result_df['BidEdge'] = pd.to_numeric(result_df['BidEdge'], errors='coerce')\n",
    "    verticals_df = result_df[result_df['sprdtype'].str.contains(\"Vertical\")].copy()\n",
    "    verticals_df['prc_eqt_at_time'] = pd.to_numeric(verticals_df['prc_eqt_at_time'], errors='coerce')\n",
    "    verticals_df['price_at_915'] = pd.to_numeric(verticals_df['price_at_915'], errors='coerce')\n",
    "    verticals_df['equity_change_pct'] = ((verticals_df['price_at_915'] - verticals_df['prc_eqt_at_time']) / verticals_df['prc_eqt_at_time']) * 100\n",
    "    verticals_df.drop(columns=['rate1', 'rate2', 'vol1', 'vol2', 'BS_Price1', 'BS_Price2'], inplace=True)\n",
    "    verticals_df.to_csv(f\"verticals_{date}.csv\")\n",
    "    verticles_set1 = set1(verticals_df)\n",
    "    verticles_set1.to_csv(f\"verticles_set_1_{date}.csv\")\n",
    "    verticles_set2 = set2(verticals_df)\n",
    "    verticles_set2.to_csv(f\"verticles_set_2_{date}.csv\")\n",
    "    verticles_set3 = verticals_df[(verticals_df['BidEdge'] > 0.50) | (verticals_df['OfferEdge'] > 0.50)]\n",
    "    verticles_set3.to_csv(f\"verticles_set_3_{date}.csv\")\n",
    "    straddle_df = result_df[result_df['sprdtype'].str.contains(\"Straddle\")].copy()\n",
    "    straddle_df.to_csv(f\"straddles_{date}.csv\")\n",
    "    strangle_df = result_df[result_df['sprdtype'].str.contains(\"Strangle\")].copy()\n",
    "    strangle_df.to_csv(f\"strangles_{date}.csv\")\n",
    "    end_time_7 = time.perf_counter()\n",
    "    print(f\"time7: {end_time_7 - start_time_7}\")\n",
    "\n",
    "    zip_filename = f\"e.watchlist_{date}.zip\"\n",
    "    straddle_csv = f\"straddles_{date}.csv\"\n",
    "    vertical_csv = f\"verticals_{date}.csv\"\n",
    "    verticles_set1 = f\"verticles_set_1_{date}.csv\"\n",
    "    verticles_set2 = f\"verticles_set_2_{date}.csv\"\n",
    "    verticles_set3 = f\"verticles_set_3_{date}.csv\"\n",
    "    strangles_csv = f\"strangles_{date}.csv\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        zip_file.write(straddle_csv, arcname=os.path.basename(straddle_csv))\n",
    "        zip_file.write(vertical_csv, arcname=os.path.basename(vertical_csv))\n",
    "        zip_file.write(verticles_set1, arcname=os.path.basename(verticles_set1))\n",
    "        zip_file.write(verticles_set2, arcname=os.path.basename(verticles_set2))\n",
    "        zip_file.write(verticles_set3, arcname=os.path.basename(verticles_set3))\n",
    "        zip_file.write(strangles_csv, arcname=os.path.basename(strangles_csv))\n",
    "\n",
    "    #Send the zip archive via email\n",
    "    send_email(['ewashington@scalptrade.com', 'sleland@scalptrade.com', 'aiacullo@scalptrade.com', 'jfeng@scalptrade.com', 'jthakkar@scalptrade.com ', 'jwood@scalptrade.com'], \n",
    "            'Open Rotation Watchlist', \n",
    "            attachment_path=zip_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.perf_counter()\n",
    "    for i in range(40)\n",
    "        run()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Total runtime: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1: 0.7300206760410219\n",
      "time2: 11.038635996053927\n",
      "time3: 19.872425372013822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 565\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    564\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m--> 565\u001b[0m     run()\n\u001b[1;32m    566\u001b[0m     end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    567\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal runtime: \u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 472\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39m# Measure time for get_batch_data1\u001b[39;00m\n\u001b[1;32m    471\u001b[0m start_time_get_batch \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m--> 472\u001b[0m pricesattime_df \u001b[39m=\u001b[39m get_batch_data1_parallel(merged_df)\n\u001b[1;32m    473\u001b[0m end_time_get_batch \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    474\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime taken for get_batch_data1: \u001b[39m\u001b[39m{\u001b[39;00mend_time_get_batch\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time_get_batch\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 386\u001b[0m, in \u001b[0;36mget_batch_data1_parallel\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    384\u001b[0m unique_symbols \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39munderly\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m    385\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 386\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(executor\u001b[39m.\u001b[39;49mmap(get_batch_data1, [df[df[\u001b[39m'\u001b[39;49m\u001b[39munderly\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m symbol] \u001b[39mfor\u001b[39;49;00m symbol \u001b[39min\u001b[39;49;00m unique_symbols]))\n\u001b[1;32m    387\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat(results, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sqlalchemy import create_engine\n",
    "import zipfile\n",
    "\n",
    "connection = psycopg2.connect(host=\"10.5.1.20\", database=\"marketdata\", user=\"elliott\", password=\"scalp\", port='5432')\n",
    "connection2 = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "\n",
    "m_root = 'RBadmin'\n",
    "m_password = '$Calp123'\n",
    "m_host = '10.5.1.32'\n",
    "m_db = 'rbandits2'\n",
    "uri = f\"mysql+mysqldb://{m_root}:{m_password}@{m_host}/{m_db}\"\n",
    "mydb = create_engine(uri, connect_args={'ssl_mode': 'DISABLED'})\n",
    "mapping = {0: \"Vertical\",5: \"Straddle\", 6: \"Strangle\"}\n",
    "\n",
    "def identify_inverted(row):\n",
    "    # Ignore the rows where either 'BBOAsk' or 'BBOBid' is zero\n",
    "    if row['BBOAsk'] == 0 or row['BBOBid'] == 0:\n",
    "        return ''\n",
    "    # Check if 'BBOAsk' is less than 'BBOBid'\n",
    "    elif row['BBOAsk'] < row['BBOBid']:\n",
    "        return 'Inverted'\n",
    "    else:\n",
    "        return ''\n",
    "def identify_option_type(row):\n",
    "    # Check if the 'sprdtype' is 'Vertical'\n",
    "    if row['sprdtype'] == 'Vertical':\n",
    "        # Check for 'C' or 'P' surrounded by digits in 'formatted_symbol'\n",
    "        matches = re.findall(r'\\d(C|P)\\d', row['formatted_symbol'])\n",
    "        if matches:\n",
    "            # Take the first match and check the middle character\n",
    "            if 'C' in matches[0]:\n",
    "                return 'Call Vertical'\n",
    "            else:\n",
    "                \n",
    "                return 'Put Vertical'\n",
    "    # If 'sprdtype' is not 'Vertical', return the original 'sprdtype'\n",
    "    return row['sprdtype']\n",
    "def get_data(date):\n",
    "    query = f\"\"\"\n",
    "                SELECT ts as Time, underly, sprdsym as Spread, price as LastPrice, sprdtype\n",
    "                FROM trdsprd\n",
    "                WHERE ts >= DATE_SUB('{date}', INTERVAL 1 DAY)\n",
    "                AND DAYOFWEEK(ts) BETWEEN 2 AND 6\n",
    "                AND sprdtype in ('0', '5', '6')\n",
    "                AND underly NOT LIKE 'QQQ%%'\n",
    "                AND underly NOT LIKE 'SPY%%'\n",
    "                AND underly NOT LIKE 'IWM%%'\n",
    "                ORDER BY ts DESC\n",
    "            \"\"\"\n",
    "    df = pd.read_sql_query(query, mydb)\n",
    "    return df\n",
    "def convert_to_new_format(option):\n",
    "    if pd.isnull(option):\n",
    "        return ''  # Return an empty string for NaN values\n",
    "\n",
    "    parts = option.split('_')  # Split the spread into individual options\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        # Identify the beginning of the date substring\n",
    "        date_start = None\n",
    "        for i in range(len(part) - 5):  # Subtract 5 to avoid running off the end of the string\n",
    "            if part[i:i+2].isdigit() and part[i+2:i+4].isdigit() and part[i+4:i+6].isdigit():\n",
    "                date_start = i\n",
    "                break\n",
    "        # If a date substring couldn't be found, treat the part as a non-option\n",
    "        if date_start is None:\n",
    "            new_parts.append(part)\n",
    "            continue\n",
    "        # Extract underlying, date, call/put, strike, and suffix\n",
    "        underlying = part[:date_start]\n",
    "        date = '20' + part[date_start:date_start+6]  # Convert YY to YYYY\n",
    "        cp = part[date_start+6]\n",
    "        strike_start = date_start + 7\n",
    "        strike_end = strike_start\n",
    "        for char in part[strike_start:]:\n",
    "            if char.isdigit() or char == '.':\n",
    "                strike_end += 1\n",
    "            else:\n",
    "                break\n",
    "        strike = part[strike_start:strike_end]\n",
    "        # Append decimal and trailing zeros if necessary\n",
    "        if '.' not in strike:\n",
    "            strike += '.00'\n",
    "        elif len(strike.split('.')[1]) == 1:\n",
    "            strike += '0'\n",
    "        suffix = part[strike_end:]\n",
    "        new_part = underlying + date + cp + strike + suffix\n",
    "        new_parts.append(new_part)\n",
    "    return '_'.join(new_parts)  # Join the options back into a spread\n",
    "def get_batch_data2(df, current_date):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    symbols = df['underly'].unique()\n",
    "    \n",
    "    # Convert the current date string to a datetime.date object\n",
    "    current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "    \n",
    "    # Calculate the target times\n",
    "    target_time_915 = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "    target_time_914 = datetime.combine(current_date_obj, pd.Timestamp(\"9:14:00\").time())\n",
    "    \n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    (tradets BETWEEN %s AND %s)\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, target_time_914, target_time_915))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def get_batch_data1(df):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    min_time = df['Time'].min()\n",
    "    max_time = df['Time'].max()\n",
    "    symbols = df['underly'].unique()\n",
    "\n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    tradets BETWEEN %s AND %s\n",
    "            \"\"\"\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, min_time, max_time))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def assign_closest_prices(original_df, batch_data_df):\n",
    "    # Create a new column for prc_eqt_at_time\n",
    "    original_df = original_df.copy() \n",
    "    original_df['prc_eqt_at_time'] = None\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in original_df['underly'].unique():\n",
    "        # Work on a copy of the slice\n",
    "        original_subset = original_df[original_df['underly'] == symbol].copy()\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "        \n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "\n",
    "        # For each row in the original subset, find the closest timestamp in the batch subset\n",
    "        for idx, row in original_subset.iterrows():\n",
    "            time_diffs = (batch_subset['Timestamp'] - row['Time']).abs()\n",
    "            \n",
    "            # If there are no time differences, skip this row\n",
    "            if time_diffs.empty:\n",
    "                continue\n",
    "\n",
    "            closest_idx = time_diffs.idxmin()\n",
    "            closest_trade_price = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "\n",
    "            # Assign the closest trade price to the prc_eqt_at_time column\n",
    "            original_subset.at[idx, 'prc_eqt_at_time'] = closest_trade_price\n",
    "\n",
    "        # Update the original DataFrame\n",
    "        original_df.update(original_subset)\n",
    "\n",
    "    return original_df\n",
    "def send_email(names, subject, attachment_path):\n",
    "    fromaddr = \"reports@scalptrade.com\"\n",
    "    toaddr = names\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = fromaddr\n",
    "    msg['To'] = \", \".join(toaddr)\n",
    "    msg['Subject'] = '{}'.format(subject)\n",
    "\n",
    "    # Attach the CSV file\n",
    "    attachment = open(attachment_path, 'rb')\n",
    "    part = MIMEBase('application', 'octet-stream')\n",
    "    part.set_payload(attachment.read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(attachment_path)}\"')\n",
    "    msg.attach(part)\n",
    "\n",
    "    s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    s.starttls()\n",
    "    s.login(fromaddr, \"sc@lptrade\")\n",
    "    text = msg.as_string()\n",
    "    s.sendmail(fromaddr, toaddr, text)\n",
    "def round_to_two_decimals(df, columns):\n",
    "    \"\"\"Format specified columns in a dataframe to two decimal places as strings.\"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: \"{:.2f}\".format(float(x)) if pd.notna(x) else None)\n",
    "    return df\n",
    "def assign_equity_prices_at_915(original_df, batch_data_df, current_date):\n",
    "    # Create a new column for price at 915\n",
    "    original_df['price_at_915'] = None\n",
    "    \n",
    "    # Get a unique set of symbols\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in unique_symbols:\n",
    "        # Filter batch data for the current symbol\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "\n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "        \n",
    "        # Convert the current_date string to a datetime.date object\n",
    "        current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "        \n",
    "        # Calculate the target time\n",
    "        target_time = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "        \n",
    "        time_diffs = batch_subset['Timestamp'].apply(lambda x: abs(datetime.combine(current_date_obj, x.time()) - target_time))\n",
    "        closest_idx = time_diffs.idxmin()\n",
    "        price_at_915 = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "                \n",
    "        # Assign the price to all rows with the matching symbol\n",
    "        original_df.loc[original_df['underly'] == symbol, 'price_at_915'] = price_at_915\n",
    "    return original_df\n",
    "def generate_and_execute_sql(result_df):\n",
    "    new_df = result_df[['Symbol', 'underly']].copy()\n",
    "    new_df['Symbol'] = new_df['Symbol'].str.split('_')\n",
    "    exploded_df = new_df.explode('Symbol', ignore_index=True)\n",
    "    unique_underly_values = exploded_df['underly'].unique()\n",
    "\n",
    "    unique_underly_str = ','.join(f\"'{symbol}%'\" for symbol in unique_underly_values)\n",
    "    unique_underly_str = f\"ARRAY[{unique_underly_str}]\"\n",
    "\n",
    "    # Database connection\n",
    "    connection = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT theo_rate, brents_vol, s.symbol, underlying\n",
    "        FROM public.theo_rates r, public.theo_opt_sym_chain s\n",
    "        WHERE r.theo_symbol_id = s.theo_symbol_id\n",
    "        AND symbol LIKE ANY({unique_underly_str})\n",
    "        AND r.create_time > timestamp '2023-08-28 16:00:00'\n",
    "        ORDER BY strike, r.create_time DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    query_results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    df = pd.DataFrame(query_results, columns=['theo_rate', 'volatility', 'symbol', 'underlying'])\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        symbol = row['symbol']\n",
    "        theo_rate = row['theo_rate']\n",
    "        volatility = row['volatility']\n",
    "    \n",
    "        result_dict[symbol] = {'theo_rate': theo_rate, 'volatility': volatility}\n",
    "\n",
    "    return result_dict\n",
    "def lookup_rates_and_vols(row, rates_vol_dict):\n",
    "    symbols = row['Symbol'].split('_')\n",
    "    modified_symbols = [symbol[:-2] for symbol in symbols]\n",
    "    \n",
    "    if len(modified_symbols) != 2:\n",
    "        #print(f\"Skipping row with symbols: {modified_symbols}\")\n",
    "        return row  # or set specific columns to NaN or some default value\n",
    "    \n",
    "    try:\n",
    "        symbol1, symbol2 = modified_symbols\n",
    "        row['rate1'] = rates_vol_dict[symbol1]['theo_rate']\n",
    "        row['rate2'] = rates_vol_dict[symbol2]['theo_rate']\n",
    "        row['vol1'] = rates_vol_dict[symbol1]['volatility']\n",
    "        row['vol2'] = rates_vol_dict[symbol2]['volatility']\n",
    "    except KeyError:\n",
    "        #print(f\"Symbols {modified_symbols} not found in rates_vol_dict.\")\n",
    "        row['rate1'], row['rate2'], row['vol1'], row['vol2'] = [np.nan]*4\n",
    "    \n",
    "    return row\n",
    "def black_scholes(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    d2 = d1 - sigma * np.sqrt(t)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        option_price = S * np.exp(-q * t) * stats.norm.cdf(d1) - X * np.exp(-r * t) * stats.norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        option_price = X * np.exp(-r * t) * stats.norm.cdf(-d2) - S * np.exp(-q * t) * stats.norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "        \n",
    "    return option_price\n",
    "def black_scholes_delta(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        delta = np.exp(-q * t) * stats.norm.cdf(d1)\n",
    "    elif option_type == 'put':\n",
    "        delta = np.exp(-q * t) * (stats.norm.cdf(d1) - 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "    \n",
    "    return delta\n",
    "def calculate_black_scholes_and_delta(row):\n",
    "    S = float(row['prc_eqt_at_time'])  # Convert to float\n",
    "    X1 = float(row['Symbol'].split('_')[0].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    X2 = float(row['Symbol'].split('_')[1].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    t = 30 / 365  # Assuming 30 days to expiration (replace with actual value)\n",
    "    r1 = row['rate1']\n",
    "    r2 = row['rate2']\n",
    "    q = 0  # Assuming no dividend yield\n",
    "    sigma1 = row['vol1']\n",
    "    sigma2 = row['vol2']\n",
    "    \n",
    "    bs_price1 = black_scholes(S, X1, t, r1, q, sigma1)\n",
    "    bs_price2 = black_scholes(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    delta1 = black_scholes_delta(S, X1, t, r1, q, sigma1)\n",
    "    delta2 = black_scholes_delta(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    row['BS_Price1'] = bs_price1\n",
    "    row['BS_Price2'] = bs_price2\n",
    "    row['Delta1'] = delta1\n",
    "    row['Delta2'] = delta2\n",
    "    row['Spread_Delta'] = delta1 - delta2\n",
    "    \n",
    "    return row\n",
    "def process_put_verticals(row):\n",
    "    if row['sprdtype'] == 'Put Vertical':\n",
    "        # Swap BBOAsk and BBOBid\n",
    "        row['BBOAsk'], row['BBOBid'] = row['BBOBid'], row['BBOAsk']\n",
    "        \n",
    "        # Swap BBOAskSize and BBOBidSize (assuming you have these columns)\n",
    "        row['BBOAskSize'], row['BBOBidSize'] = row['BBOBidSize'], row['BBOAskSize']\n",
    "\n",
    "        # Take absolute values of LastPrice, BBOBid, and BBOAsk\n",
    "        row['LastPrice'] = abs(row['LastPrice'])\n",
    "        row['BBOBid'] = abs(row['BBOBid'])\n",
    "        row['BBOAsk'] = abs(row['BBOAsk'])\n",
    "        \n",
    "    return row\n",
    "def get_data_parallel(dates):\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        results = list(executor.map(get_data, dates))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def get_batch_data1_parallel(df):\n",
    "    unique_symbols = df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        results = list(executor.map(get_batch_data1, [df[df['underly'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def assign_closest_prices_parallel(original_df, batch_data_df):\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        results = list(executor.map(assign_closest_prices, \n",
    "                                    [original_df[original_df['underly'] == symbol] for symbol in unique_symbols],\n",
    "                                    [batch_data_df[batch_data_df['Symbol'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def generate_and_execute_sql_parallel(result_df):\n",
    "    unique_underly_values = result_df['underly'].unique()\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        results = list(executor.map(generate_and_execute_sql, [result_df[result_df['underly'] == symbol] for symbol in unique_underly_values]))\n",
    "    return {k: v for d in results for k, v in d.items()}\n",
    "def set1(df):\n",
    "    condition1 = (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    condition2 = (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    return df[condition1 | condition2]\n",
    "def set2(df):\n",
    "    condition = (\n",
    "        df['equity_change_pct'].abs() > 1\n",
    "    ) & ((df['Spread_Delta'].abs() > .25))\n",
    "    return df[condition]\n",
    "def run():\n",
    "    # Time Block 1\n",
    "    start_time_1 = time.perf_counter()\n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    dtype_map = {'Symbol': str, 'BBOAsk': float, 'BBOBid': float, 'BBOBidSize': int, 'BBOAskSize': int}\n",
    "    main_df = pd.read_csv(f'/home/elliott/Development/scripts/jupyter_notebooks/spreads-{date}0915.csv', usecols=['Symbol', 'BBOAsk', 'BBOBid', 'BBOBidSize', 'BBOAskSize'], dtype=dtype_map)\n",
    "    main_df = main_df[(main_df['BBOBid'] != 0) | (main_df['BBOAsk'] != 0)]\n",
    "    \n",
    "    # Assuming convert_to_new_format is a function you have defined elsewhere\n",
    "    main_df['formatted_symbol'] = main_df['Symbol'].apply(convert_to_new_format)\n",
    "    \n",
    "    end_time_1 = time.perf_counter()\n",
    "    print(f\"time1: {end_time_1 - start_time_1}\")\n",
    "\n",
    "    # Time Block 2\n",
    "    start_time_2 = time.perf_counter()\n",
    "    \n",
    "    # Assuming get_data_parallel is a function you have defined elsewhere\n",
    "    db_df = get_data_parallel([date])\n",
    "    merged_df = main_df.merge(db_df, how='left', left_on='formatted_symbol', right_on='Spread')\n",
    "    merged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Assuming mapping is a dictionary you have defined elsewhere\n",
    "    merged_df['sprdtype'].replace(mapping, inplace=True)\n",
    "    \n",
    "    # Assuming identify_option_type, identify_inverted, and process_put_verticals are functions you have defined elsewhere\n",
    "    merged_df['sprdtype'] = merged_df.apply(identify_option_type, axis=1)\n",
    "    merged_df['Inverted'] = merged_df.apply(identify_inverted, axis=1)\n",
    "    merged_df.apply(process_put_verticals, axis=1)\n",
    "    \n",
    "    merged_df.sort_values(by='Time', ascending=False, inplace=True)\n",
    "    merged_df.drop_duplicates(subset='formatted_symbol', keep='first', inplace=True)\n",
    "    \n",
    "    end_time_2 = time.perf_counter()\n",
    "    print(f\"time2: {end_time_2 - start_time_2}\")\n",
    "\n",
    "    start_time_3 = time.perf_counter()\n",
    "    prices915_df = get_batch_data2(merged_df,date)\n",
    "    assign_equity_prices_at_915(merged_df, prices915_df, date)  # This line was added\n",
    "    end_time_3 = time.perf_counter()\n",
    "    print(f\"time3: {end_time_3 - start_time_3}\")\n",
    "\n",
    "    # Measure time for get_batch_data1\n",
    "    start_time_get_batch = time.perf_counter()\n",
    "    pricesattime_df = get_batch_data1_parallel(merged_df)\n",
    "    end_time_get_batch = time.perf_counter()\n",
    "    print(f\"Time taken for get_batch_data1: {end_time_get_batch - start_time_get_batch} seconds\")\n",
    "\n",
    "    # Measure time for assign_closest_prices\n",
    "    start_time_assign_closest = time.perf_counter()\n",
    "    merged_prices_df = assign_closest_prices_parallel(merged_df, pricesattime_df)\n",
    "    merged_prices_df['equity_change_pct'] = ((merged_prices_df['prc_eqt_at_time'] - merged_prices_df['price_at_915']) / merged_prices_df['price_at_915']) * 100\n",
    "    end_time_assign_closest = time.perf_counter()\n",
    "    print(f\"Time taken for assign_closest_prices: {end_time_assign_closest - start_time_assign_closest} seconds\")\n",
    "\n",
    "\n",
    "    start_time_5 = time.perf_counter()\n",
    "    column_order = ['formatted_symbol', 'Symbol', 'Inverted', 'sprdtype', 'LastPrice', 'BBOBid', 'BBOBidSize',\n",
    "                    'BBOAsk', 'BBOAskSize', 'Time', 'underly', 'prc_eqt_at_time', 'price_at_915']\n",
    "    result_df = (merged_prices_df.loc[:, column_order]\n",
    "                .dropna(subset=['price_at_915', 'prc_eqt_at_time'])\n",
    "                .assign(PriceDelta=lambda df: df['price_at_915'].astype(float) - df['prc_eqt_at_time'],\n",
    "                        BidEdge=lambda df: np.where(df['BBOBid'] != 0, df['BBOBid'] - df['LastPrice'], np.nan),\n",
    "                        OfferEdge=lambda df: np.where(df['BBOAsk'] != 0, df['LastPrice'] - df['BBOAsk'], np.nan))\n",
    "            ).copy()\n",
    "    result_df = round_to_two_decimals(result_df, columns=['BidEdge', 'OfferEdge', 'prc_eqt_at_time', 'price_at_915', \n",
    "                                                        'PriceDelta', 'LastPrice', 'BBOBid', 'BBOAsk'])\n",
    "    result_df['BidEdge'] = result_df['BidEdge'].astype(float)\n",
    "    result_df['OfferEdge'] = result_df['OfferEdge'].astype(float)\n",
    "    result_df = result_df.loc[(result_df['BidEdge'] < -0.20) | (result_df['OfferEdge'] < -0.20)]\n",
    "    end_time_5 = time.perf_counter()\n",
    "    print(f\"time5: {end_time_5 - start_time_5}\")\n",
    "\n",
    "    start_time_6_1 = time.perf_counter()\n",
    "    vol_rates_dict = generate_and_execute_sql_parallel(result_df)\n",
    "    end_time_6_1 = time.perf_counter()\n",
    "    print(f\"Time taken to generate_and_execute_sql_parallel: {end_time_6_1 - start_time_6_1} seconds\")\n",
    "\n",
    "    start_time_6_2 = time.perf_counter()\n",
    "    result_df = result_df.apply(lookup_rates_and_vols, axis=1, args=(vol_rates_dict,))\n",
    "    result_df.dropna(subset=['rate1'], inplace=True)\n",
    "    result_df = result_df.apply(calculate_black_scholes_and_delta, axis=1)\n",
    "    end_time_6_2 = time.perf_counter()\n",
    "    print(f\"time6.2: {end_time_6_2 - start_time_6_2}\")\n",
    "    \n",
    "\n",
    "    start_time_7 = time.perf_counter()\n",
    "    result_df.drop(columns=[\"Symbol\"], inplace=True)\n",
    "    result_df['PriceDelta'] = pd.to_numeric(result_df['PriceDelta'], errors='coerce')\n",
    "    result_df['OfferEdge'] = pd.to_numeric(result_df['OfferEdge'], errors='coerce')\n",
    "    result_df['BidEdge'] = pd.to_numeric(result_df['BidEdge'], errors='coerce')\n",
    "    verticals_df = result_df[result_df['sprdtype'].str.contains(\"Vertical\")].copy()\n",
    "    verticals_df['prc_eqt_at_time'] = pd.to_numeric(verticals_df['prc_eqt_at_time'], errors='coerce')\n",
    "    verticals_df['price_at_915'] = pd.to_numeric(verticals_df['price_at_915'], errors='coerce')\n",
    "    verticals_df['equity_change_pct'] = ((verticals_df['price_at_915'] - verticals_df['prc_eqt_at_time']) / verticals_df['prc_eqt_at_time']) * 100\n",
    "    verticals_df.drop(columns=['rate1', 'rate2', 'vol1', 'vol2', 'BS_Price1', 'BS_Price2'], inplace=True)\n",
    "    verticals_df.to_csv(f\"verticals_{date}.csv\")\n",
    "    verticles_set1 = set1(verticals_df)\n",
    "    verticles_set1.to_csv(f\"verticles_set_1_{date}.csv\")\n",
    "    verticles_set2 = set2(verticals_df)\n",
    "    verticles_set2.to_csv(f\"verticles_set_2_{date}.csv\")\n",
    "    verticles_set3 = verticals_df[(verticals_df['BidEdge'] > 0.50) | (verticals_df['OfferEdge'] > 0.50)]\n",
    "    verticles_set3.to_csv(f\"verticles_set_3_{date}.csv\")\n",
    "    straddle_df = result_df[result_df['sprdtype'].str.contains(\"Straddle\")].copy()\n",
    "    straddle_df.to_csv(f\"straddles_{date}.csv\")\n",
    "    strangle_df = result_df[result_df['sprdtype'].str.contains(\"Strangle\")].copy()\n",
    "    strangle_df.to_csv(f\"strangles_{date}.csv\")\n",
    "    end_time_7 = time.perf_counter()\n",
    "    print(f\"time7: {end_time_7 - start_time_7}\")\n",
    "\n",
    "    start_time_8 = time.perf_counter()\n",
    "    zip_filename = f\"e.watchlist_{date}.zip\"\n",
    "    straddle_csv = f\"straddles_{date}.csv\"\n",
    "    vertical_csv = f\"verticals_{date}.csv\"\n",
    "    verticles_set1 = f\"verticles_set_1_{date}.csv\"\n",
    "    verticles_set2 = f\"verticles_set_2_{date}.csv\"\n",
    "    verticles_set3 = f\"verticles_set_3_{date}.csv\"\n",
    "    strangles_csv = f\"strangles_{date}.csv\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        zip_file.write(straddle_csv, arcname=os.path.basename(straddle_csv))\n",
    "        zip_file.write(vertical_csv, arcname=os.path.basename(vertical_csv))\n",
    "        zip_file.write(verticles_set1, arcname=os.path.basename(verticles_set1))\n",
    "        zip_file.write(verticles_set2, arcname=os.path.basename(verticles_set2))\n",
    "        zip_file.write(verticles_set3, arcname=os.path.basename(verticles_set3))\n",
    "        zip_file.write(strangles_csv, arcname=os.path.basename(strangles_csv))\n",
    "    \n",
    "    end_time_8 = time.perf_counter()\n",
    "    print(f\"time8: {end_time_8 - start_time_8}\")\n",
    "\n",
    "    #Send the zip archive via email\n",
    "    # send_email(['ewashington@scalptrade.com', 'sleland@scalptrade.com', 'aiacullo@scalptrade.com', 'jfeng@scalptrade.com', 'jthakkar@scalptrade.com ', 'jwood@scalptrade.com'], \n",
    "    #         'Open Rotation Watchlist', \n",
    "    #         attachment_path=zip_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.perf_counter()\n",
    "    run()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Total runtime: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1: 0.8111178110120818\n",
      "time2: 10.887649960001\n",
      "time3: 32.07651446701493\n",
      "Time taken for get_batch_data1: 59.351601448026486 seconds\n",
      "Time taken for assign_closest_prices: 30.249853296088986 seconds\n",
      "time5: 0.03559299104381353\n",
      "Time taken to generate_and_execute_sql_parallel: 71.40892083500512 seconds\n",
      "time6.2: 9.475634948932566\n",
      "time7: 0.06371712498366833\n",
      "time8: 0.024298794101923704\n",
      "Total runtime: 214.4538668569876\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sqlalchemy import create_engine\n",
    "import zipfile\n",
    "\n",
    "connection = psycopg2.connect(host=\"10.5.1.20\", database=\"marketdata\", user=\"elliott\", password=\"scalp\", port='5432')\n",
    "connection2 = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "\n",
    "m_root = 'RBadmin'\n",
    "m_password = '$Calp123'\n",
    "m_host = '10.5.1.32'\n",
    "m_db = 'rbandits2'\n",
    "uri = f\"mysql+mysqldb://{m_root}:{m_password}@{m_host}/{m_db}\"\n",
    "mydb = create_engine(uri, connect_args={'ssl_mode': 'DISABLED'})\n",
    "mapping = {0: \"Vertical\",5: \"Straddle\", 6: \"Strangle\"}\n",
    "\n",
    "def identify_inverted(row):\n",
    "    # Ignore the rows where either 'BBOAsk' or 'BBOBid' is zero\n",
    "    if row['BBOAsk'] == 0 or row['BBOBid'] == 0:\n",
    "        return ''\n",
    "    # Check if 'BBOAsk' is less than 'BBOBid'\n",
    "    elif row['BBOAsk'] < row['BBOBid']:\n",
    "        return 'Inverted'\n",
    "    else:\n",
    "        return ''\n",
    "def identify_option_type(row):\n",
    "    # Check if the 'sprdtype' is 'Vertical'\n",
    "    if row['sprdtype'] == 'Vertical':\n",
    "        # Check for 'C' or 'P' surrounded by digits in 'formatted_symbol'\n",
    "        matches = re.findall(r'\\d(C|P)\\d', row['formatted_symbol'])\n",
    "        if matches:\n",
    "            # Take the first match and check the middle character\n",
    "            if 'C' in matches[0]:\n",
    "                return 'Call Vertical'\n",
    "            else:\n",
    "                \n",
    "                return 'Put Vertical'\n",
    "    # If 'sprdtype' is not 'Vertical', return the original 'sprdtype'\n",
    "    return row['sprdtype']\n",
    "def get_data(date):\n",
    "    query = f\"\"\"\n",
    "                SELECT ts as Time, underly, sprdsym as Spread, price as LastPrice, sprdtype\n",
    "                FROM trdsprd\n",
    "                WHERE ts >= DATE_SUB('{date}', INTERVAL 1 DAY)\n",
    "                AND DAYOFWEEK(ts) BETWEEN 2 AND 6\n",
    "                AND sprdtype in ('0', '5', '6')\n",
    "                AND underly NOT LIKE 'QQQ%%'\n",
    "                AND underly NOT LIKE 'SPY%%'\n",
    "                AND underly NOT LIKE 'IWM%%'\n",
    "                ORDER BY ts DESC\n",
    "            \"\"\"\n",
    "    df = pd.read_sql_query(query, mydb)\n",
    "    return df\n",
    "def convert_to_new_format(option):\n",
    "    if pd.isnull(option):\n",
    "        return ''  # Return an empty string for NaN values\n",
    "\n",
    "    parts = option.split('_')  # Split the spread into individual options\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        # Identify the beginning of the date substring\n",
    "        date_start = None\n",
    "        for i in range(len(part) - 5):  # Subtract 5 to avoid running off the end of the string\n",
    "            if part[i:i+2].isdigit() and part[i+2:i+4].isdigit() and part[i+4:i+6].isdigit():\n",
    "                date_start = i\n",
    "                break\n",
    "        # If a date substring couldn't be found, treat the part as a non-option\n",
    "        if date_start is None:\n",
    "            new_parts.append(part)\n",
    "            continue\n",
    "        # Extract underlying, date, call/put, strike, and suffix\n",
    "        underlying = part[:date_start]\n",
    "        date = '20' + part[date_start:date_start+6]  # Convert YY to YYYY\n",
    "        cp = part[date_start+6]\n",
    "        strike_start = date_start + 7\n",
    "        strike_end = strike_start\n",
    "        for char in part[strike_start:]:\n",
    "            if char.isdigit() or char == '.':\n",
    "                strike_end += 1\n",
    "            else:\n",
    "                break\n",
    "        strike = part[strike_start:strike_end]\n",
    "        # Append decimal and trailing zeros if necessary\n",
    "        if '.' not in strike:\n",
    "            strike += '.00'\n",
    "        elif len(strike.split('.')[1]) == 1:\n",
    "            strike += '0'\n",
    "        suffix = part[strike_end:]\n",
    "        new_part = underlying + date + cp + strike + suffix\n",
    "        new_parts.append(new_part)\n",
    "    return '_'.join(new_parts)  # Join the options back into a spread\n",
    "def get_batch_data2(df, current_date):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    symbols = df['underly'].unique()\n",
    "    \n",
    "    # Convert the current date string to a datetime.date object\n",
    "    current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "    \n",
    "    # Calculate the target times\n",
    "    target_time_915 = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "    target_time_914 = datetime.combine(current_date_obj, pd.Timestamp(\"9:14:00\").time())\n",
    "    \n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    (tradets BETWEEN %s AND %s)\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, target_time_914, target_time_915))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame.from_records(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def get_batch_data1(df):\n",
    "    # Extract relevant parameters from the dataframe\n",
    "    min_time = df['Time'].min()\n",
    "    max_time = df['Time'].max()\n",
    "    symbols = df['underly'].unique()\n",
    "\n",
    "    # Convert symbols list to a format suitable for SQL IN clause\n",
    "    symbols_placeholder = \",\".join([\"%s\"] * len(symbols))\n",
    "\n",
    "    # Construct the query\n",
    "    query = f\"\"\"\n",
    "                SELECT symbol, tradets, tradevolume, tradelast\n",
    "                FROM equity_trades_new\n",
    "                WHERE \n",
    "                    symbol IN ({symbols_placeholder}) AND\n",
    "                    tradets BETWEEN %s AND %s\n",
    "            \"\"\"\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the query using parameters\n",
    "    cursor.execute(query, (*symbols, min_time, max_time))\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    cursor.close()\n",
    "\n",
    "    # Convert results to a dataframe for easier manipulation\n",
    "    columns = ['Symbol', 'Timestamp', 'Volume', 'Last_Trade']\n",
    "    batch_data_df = pd.DataFrame.from_records(results, columns=columns)\n",
    "\n",
    "    return batch_data_df\n",
    "def assign_closest_prices(original_df, batch_data_df):\n",
    "    # Create a new column for prc_eqt_at_time\n",
    "    original_df = original_df.copy() \n",
    "    original_df['prc_eqt_at_time'] = None\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in original_df['underly'].unique():\n",
    "        # Work on a copy of the slice\n",
    "        original_subset = original_df[original_df['underly'] == symbol].copy()\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "        \n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "\n",
    "        # For each row in the original subset, find the closest timestamp in the batch subset\n",
    "        for idx, row in original_subset.iterrows():\n",
    "            time_diffs = (batch_subset['Timestamp'] - row['Time']).abs()\n",
    "            \n",
    "            # If there are no time differences, skip this row\n",
    "            if time_diffs.empty:\n",
    "                continue\n",
    "\n",
    "            closest_idx = time_diffs.idxmin()\n",
    "            closest_trade_price = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "\n",
    "            # Assign the closest trade price to the prc_eqt_at_time column\n",
    "            original_subset.at[idx, 'prc_eqt_at_time'] = closest_trade_price\n",
    "\n",
    "        # Update the original DataFrame\n",
    "        original_df.update(original_subset)\n",
    "\n",
    "    return original_df\n",
    "def send_email(names, subject, attachment_path):\n",
    "    fromaddr = \"reports@scalptrade.com\"\n",
    "    toaddr = names\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = fromaddr\n",
    "    msg['To'] = \", \".join(toaddr)\n",
    "    msg['Subject'] = '{}'.format(subject)\n",
    "\n",
    "    # Attach the CSV file\n",
    "    attachment = open(attachment_path, 'rb')\n",
    "    part = MIMEBase('application', 'octet-stream')\n",
    "    part.set_payload(attachment.read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(attachment_path)}\"')\n",
    "    msg.attach(part)\n",
    "\n",
    "    s = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    s.starttls()\n",
    "    s.login(fromaddr, \"sc@lptrade\")\n",
    "    text = msg.as_string()\n",
    "    s.sendmail(fromaddr, toaddr, text)\n",
    "def round_to_two_decimals(df, columns):\n",
    "    \"\"\"Format specified columns in a dataframe to two decimal places as strings.\"\"\"\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: \"{:.2f}\".format(float(x)) if pd.notna(x) else None)\n",
    "    return df\n",
    "def assign_equity_prices_at_915(original_df, batch_data_df, current_date):\n",
    "    # Create a new column for price at 915\n",
    "    original_df['price_at_915'] = None\n",
    "    \n",
    "    # Get a unique set of symbols\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "\n",
    "    # Process each unique symbol\n",
    "    for symbol in unique_symbols:\n",
    "        # Filter batch data for the current symbol\n",
    "        batch_subset = batch_data_df[batch_data_df['Symbol'] == symbol]\n",
    "\n",
    "        # If there are no matching records in batch_subset, skip this symbol\n",
    "        if batch_subset.empty:\n",
    "            continue\n",
    "        \n",
    "        # Convert the current_date string to a datetime.date object\n",
    "        current_date_obj = datetime.strptime(current_date, '%Y%m%d').date()\n",
    "        \n",
    "        # Calculate the target time\n",
    "        target_time = datetime.combine(current_date_obj, pd.Timestamp(\"9:15:00\").time())\n",
    "        \n",
    "        time_diffs = batch_subset['Timestamp'].apply(lambda x: abs(datetime.combine(current_date_obj, x.time()) - target_time))\n",
    "        closest_idx = time_diffs.idxmin()\n",
    "        price_at_915 = batch_subset.loc[closest_idx, 'Last_Trade']\n",
    "                \n",
    "        # Assign the price to all rows with the matching symbol\n",
    "        original_df.loc[original_df['underly'] == symbol, 'price_at_915'] = price_at_915\n",
    "    return original_df\n",
    "def generate_and_execute_sql(result_df):\n",
    "    new_df = result_df[['Symbol', 'underly']].copy()\n",
    "    new_df['Symbol'] = new_df['Symbol'].str.split('_')\n",
    "    exploded_df = new_df.explode('Symbol', ignore_index=True)\n",
    "    unique_underly_values = exploded_df['underly'].unique()\n",
    "\n",
    "    unique_underly_str = ','.join(f\"'{symbol}%'\" for symbol in unique_underly_values)\n",
    "    unique_underly_str = f\"ARRAY[{unique_underly_str}]\"\n",
    "\n",
    "    # Database connection\n",
    "    connection = psycopg2.connect(host=\"10.7.8.59\", database=\"theoretical\", user=\"scalp\", password=\"QAtr@de442\", port='5433')\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT theo_rate, brents_vol, s.symbol, underlying\n",
    "        FROM public.theo_rates r, public.theo_opt_sym_chain s\n",
    "        WHERE r.theo_symbol_id = s.theo_symbol_id\n",
    "        AND symbol LIKE ANY({unique_underly_str})\n",
    "        AND r.create_time > timestamp '2023-08-28 16:00:00'\n",
    "        ORDER BY strike, r.create_time DESC;\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query)\n",
    "    query_results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    df = pd.DataFrame(query_results, columns=['theo_rate', 'volatility', 'symbol', 'underlying'])\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        symbol = row['symbol']\n",
    "        theo_rate = row['theo_rate']\n",
    "        volatility = row['volatility']\n",
    "    \n",
    "        result_dict[symbol] = {'theo_rate': theo_rate, 'volatility': volatility}\n",
    "\n",
    "    return result_dict\n",
    "def lookup_rates_and_vols(row, rates_vol_dict):\n",
    "    symbols = row['Symbol'].split('_')\n",
    "    modified_symbols = [symbol[:-2] for symbol in symbols]\n",
    "    \n",
    "    if len(modified_symbols) != 2:\n",
    "        #print(f\"Skipping row with symbols: {modified_symbols}\")\n",
    "        return row  # or set specific columns to NaN or some default value\n",
    "    \n",
    "    try:\n",
    "        symbol1, symbol2 = modified_symbols\n",
    "        row['rate1'] = rates_vol_dict[symbol1]['theo_rate']\n",
    "        row['rate2'] = rates_vol_dict[symbol2]['theo_rate']\n",
    "        row['vol1'] = rates_vol_dict[symbol1]['volatility']\n",
    "        row['vol2'] = rates_vol_dict[symbol2]['volatility']\n",
    "    except KeyError:\n",
    "        #print(f\"Symbols {modified_symbols} not found in rates_vol_dict.\")\n",
    "        row['rate1'], row['rate2'], row['vol1'], row['vol2'] = [np.nan]*4\n",
    "    \n",
    "    return row\n",
    "def black_scholes(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    d2 = d1 - sigma * np.sqrt(t)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        option_price = S * np.exp(-q * t) * stats.norm.cdf(d1) - X * np.exp(-r * t) * stats.norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        option_price = X * np.exp(-r * t) * stats.norm.cdf(-d2) - S * np.exp(-q * t) * stats.norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "        \n",
    "    return option_price\n",
    "def black_scholes_delta(S, X, t, r, q, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / X) + (r - q + sigma ** 2 / 2) * t) / (sigma * np.sqrt(t))\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        delta = np.exp(-q * t) * stats.norm.cdf(d1)\n",
    "    elif option_type == 'put':\n",
    "        delta = np.exp(-q * t) * (stats.norm.cdf(d1) - 1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option_type. Use 'call' or 'put'\")\n",
    "    \n",
    "    return delta\n",
    "def calculate_black_scholes_and_delta(row):\n",
    "    S = float(row['prc_eqt_at_time'])  # Convert to float\n",
    "    X1 = float(row['Symbol'].split('_')[0].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    X2 = float(row['Symbol'].split('_')[1].split('C')[-1].split('P')[-1][:-2])  # Convert to float\n",
    "    t = 30 / 365  # Assuming 30 days to expiration (replace with actual value)\n",
    "    r1 = row['rate1']\n",
    "    r2 = row['rate2']\n",
    "    q = 0  # Assuming no dividend yield\n",
    "    sigma1 = row['vol1']\n",
    "    sigma2 = row['vol2']\n",
    "    \n",
    "    bs_price1 = black_scholes(S, X1, t, r1, q, sigma1)\n",
    "    bs_price2 = black_scholes(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    delta1 = black_scholes_delta(S, X1, t, r1, q, sigma1)\n",
    "    delta2 = black_scholes_delta(S, X2, t, r2, q, sigma2)\n",
    "    \n",
    "    row['BS_Price1'] = bs_price1\n",
    "    row['BS_Price2'] = bs_price2\n",
    "    row['Delta1'] = delta1\n",
    "    row['Delta2'] = delta2\n",
    "    row['Spread_Delta'] = delta1 - delta2\n",
    "    \n",
    "    return row\n",
    "def process_put_verticals(row):\n",
    "    if row['sprdtype'] == 'Put Vertical':\n",
    "        # Swap BBOAsk and BBOBid\n",
    "        row['BBOAsk'], row['BBOBid'] = row['BBOBid'], row['BBOAsk']\n",
    "        \n",
    "        # Swap BBOAskSize and BBOBidSize (assuming you have these columns)\n",
    "        row['BBOAskSize'], row['BBOBidSize'] = row['BBOBidSize'], row['BBOAskSize']\n",
    "\n",
    "        # Take absolute values of LastPrice, BBOBid, and BBOAsk\n",
    "        row['LastPrice'] = abs(row['LastPrice'])\n",
    "        row['BBOBid'] = abs(row['BBOBid'])\n",
    "        row['BBOAsk'] = abs(row['BBOAsk'])\n",
    "        \n",
    "    return row\n",
    "def get_data_parallel(dates):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(get_data, dates))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def get_batch_data1_parallel(df):\n",
    "    unique_symbols = df['underly'].unique()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(get_batch_data1, [df[df['underly'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def assign_closest_prices_parallel(original_df, batch_data_df):\n",
    "    unique_symbols = original_df['underly'].unique()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(assign_closest_prices, \n",
    "                                    [original_df[original_df['underly'] == symbol] for symbol in unique_symbols],\n",
    "                                    [batch_data_df[batch_data_df['Symbol'] == symbol] for symbol in unique_symbols]))\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "def generate_and_execute_sql_parallel(result_df):\n",
    "    unique_underly_values = result_df['underly'].unique()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(generate_and_execute_sql, [result_df[result_df['underly'] == symbol] for symbol in unique_underly_values]))\n",
    "    return {k: v for d in results for k, v in d.items()}\n",
    "def set1(df):\n",
    "    condition1 = (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] > 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    condition2 = (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Call Vertical') & \n",
    "        (df['BidEdge'] > 0)\n",
    "    ) | (\n",
    "        (df['PriceDelta'] < 0) & \n",
    "        (df['sprdtype'] == 'Put Vertical') & \n",
    "        (df['OfferEdge'] > 0)\n",
    "    )\n",
    "    \n",
    "    return df[condition1 | condition2]\n",
    "def set2(df):\n",
    "    condition = (\n",
    "        df['equity_change_pct'].abs() > 1\n",
    "    ) & ((df['Spread_Delta'].abs() > .25))\n",
    "    return df[condition]\n",
    "def run():\n",
    "    # Time Block 1\n",
    "    start_time_1 = time.perf_counter()\n",
    "    date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    dtype_map = {'Symbol': str, 'BBOAsk': float, 'BBOBid': float, 'BBOBidSize': int, 'BBOAskSize': int}\n",
    "    main_df = pd.read_csv(f'/home/elliott/Development/scripts/jupyter_notebooks/spreads-{date}0915.csv', usecols=['Symbol', 'BBOAsk', 'BBOBid', 'BBOBidSize', 'BBOAskSize'], dtype=dtype_map)\n",
    "    main_df = main_df[(main_df['BBOBid'] != 0) | (main_df['BBOAsk'] != 0)]\n",
    "    \n",
    "    # Assuming convert_to_new_format is a function you have defined elsewhere\n",
    "    main_df['formatted_symbol'] = main_df['Symbol'].apply(convert_to_new_format)\n",
    "    \n",
    "    end_time_1 = time.perf_counter()\n",
    "    print(f\"time1: {end_time_1 - start_time_1}\")\n",
    "\n",
    "    # Time Block 2\n",
    "    start_time_2 = time.perf_counter()\n",
    "    \n",
    "    # Assuming get_data_parallel is a function you have defined elsewhere\n",
    "    db_df = get_data_parallel([date])\n",
    "    merged_df = main_df.merge(db_df, how='left', left_on='formatted_symbol', right_on='Spread')\n",
    "    merged_df.dropna(inplace=True)\n",
    "    \n",
    "    # Assuming mapping is a dictionary you have defined elsewhere\n",
    "    merged_df['sprdtype'].replace(mapping, inplace=True)\n",
    "    \n",
    "    # Assuming identify_option_type, identify_inverted, and process_put_verticals are functions you have defined elsewhere\n",
    "    merged_df['sprdtype'] = merged_df.apply(identify_option_type, axis=1)\n",
    "    merged_df['Inverted'] = merged_df.apply(identify_inverted, axis=1)\n",
    "    merged_df.apply(process_put_verticals, axis=1)\n",
    "    \n",
    "    merged_df.sort_values(by='Time', ascending=False, inplace=True)\n",
    "    merged_df.drop_duplicates(subset='formatted_symbol', keep='first', inplace=True)\n",
    "    \n",
    "    end_time_2 = time.perf_counter()\n",
    "    print(f\"time2: {end_time_2 - start_time_2}\")\n",
    "\n",
    "    start_time_3 = time.perf_counter()\n",
    "    prices915_df = get_batch_data2(merged_df,date)\n",
    "    assign_equity_prices_at_915(merged_df, prices915_df, date)  # This line was added\n",
    "    end_time_3 = time.perf_counter()\n",
    "    print(f\"time3: {end_time_3 - start_time_3}\")\n",
    "\n",
    "    # Measure time for get_batch_data1\n",
    "    start_time_get_batch = time.perf_counter()\n",
    "    pricesattime_df = get_batch_data1_parallel(merged_df)\n",
    "    end_time_get_batch = time.perf_counter()\n",
    "    print(f\"Time taken for get_batch_data1: {end_time_get_batch - start_time_get_batch} seconds\")\n",
    "\n",
    "    # Measure time for assign_closest_prices\n",
    "    start_time_assign_closest = time.perf_counter()\n",
    "    merged_prices_df = assign_closest_prices_parallel(merged_df, pricesattime_df)\n",
    "    merged_prices_df['equity_change_pct'] = ((merged_prices_df['prc_eqt_at_time'] - merged_prices_df['price_at_915']) / merged_prices_df['price_at_915']) * 100\n",
    "    end_time_assign_closest = time.perf_counter()\n",
    "    print(f\"Time taken for assign_closest_prices: {end_time_assign_closest - start_time_assign_closest} seconds\")\n",
    "\n",
    "\n",
    "    start_time_5 = time.perf_counter()\n",
    "    column_order = ['formatted_symbol', 'Symbol', 'Inverted', 'sprdtype', 'LastPrice', 'BBOBid', 'BBOBidSize',\n",
    "                    'BBOAsk', 'BBOAskSize', 'Time', 'underly', 'prc_eqt_at_time', 'price_at_915']\n",
    "    result_df = (merged_prices_df.loc[:, column_order]\n",
    "                .dropna(subset=['price_at_915', 'prc_eqt_at_time'])\n",
    "                .assign(PriceDelta=lambda df: df['price_at_915'].astype(float) - df['prc_eqt_at_time'],\n",
    "                        BidEdge=lambda df: np.where(df['BBOBid'] != 0, df['BBOBid'] - df['LastPrice'], np.nan),\n",
    "                        OfferEdge=lambda df: np.where(df['BBOAsk'] != 0, df['LastPrice'] - df['BBOAsk'], np.nan))\n",
    "            ).copy()\n",
    "    result_df = round_to_two_decimals(result_df, columns=['BidEdge', 'OfferEdge', 'prc_eqt_at_time', 'price_at_915', \n",
    "                                                        'PriceDelta', 'LastPrice', 'BBOBid', 'BBOAsk'])\n",
    "    result_df['BidEdge'] = result_df['BidEdge'].astype(float)\n",
    "    result_df['OfferEdge'] = result_df['OfferEdge'].astype(float)\n",
    "    result_df = result_df.loc[(result_df['BidEdge'] < -0.20) | (result_df['OfferEdge'] < -0.20)]\n",
    "    end_time_5 = time.perf_counter()\n",
    "    print(f\"time5: {end_time_5 - start_time_5}\")\n",
    "\n",
    "    start_time_6_1 = time.perf_counter()\n",
    "    vol_rates_dict = generate_and_execute_sql_parallel(result_df)\n",
    "    end_time_6_1 = time.perf_counter()\n",
    "    print(f\"Time taken to generate_and_execute_sql_parallel: {end_time_6_1 - start_time_6_1} seconds\")\n",
    "\n",
    "    start_time_6_2 = time.perf_counter()\n",
    "    result_df = result_df.apply(lookup_rates_and_vols, axis=1, args=(vol_rates_dict,))\n",
    "    result_df.dropna(subset=['rate1'], inplace=True)\n",
    "    result_df = result_df.apply(calculate_black_scholes_and_delta, axis=1)\n",
    "    end_time_6_2 = time.perf_counter()\n",
    "    print(f\"time6.2: {end_time_6_2 - start_time_6_2}\")\n",
    "    \n",
    "\n",
    "    start_time_7 = time.perf_counter()\n",
    "    result_df.drop(columns=[\"Symbol\"], inplace=True)\n",
    "    result_df['PriceDelta'] = pd.to_numeric(result_df['PriceDelta'], errors='coerce')\n",
    "    result_df['OfferEdge'] = pd.to_numeric(result_df['OfferEdge'], errors='coerce')\n",
    "    result_df['BidEdge'] = pd.to_numeric(result_df['BidEdge'], errors='coerce')\n",
    "    verticals_df = result_df[result_df['sprdtype'].str.contains(\"Vertical\")].copy()\n",
    "    verticals_df['prc_eqt_at_time'] = pd.to_numeric(verticals_df['prc_eqt_at_time'], errors='coerce')\n",
    "    verticals_df['price_at_915'] = pd.to_numeric(verticals_df['price_at_915'], errors='coerce')\n",
    "    verticals_df['equity_change_pct'] = ((verticals_df['price_at_915'] - verticals_df['prc_eqt_at_time']) / verticals_df['prc_eqt_at_time']) * 100\n",
    "    verticals_df.drop(columns=['rate1', 'rate2', 'vol1', 'vol2', 'BS_Price1', 'BS_Price2'], inplace=True)\n",
    "    verticals_df.to_csv(f\"verticals_{date}.csv\")\n",
    "    verticles_set1 = set1(verticals_df)\n",
    "    verticles_set1.to_csv(f\"verticles_set_1_{date}.csv\")\n",
    "    verticles_set2 = set2(verticals_df)\n",
    "    verticles_set2.to_csv(f\"verticles_set_2_{date}.csv\")\n",
    "    verticles_set3 = verticals_df[(verticals_df['BidEdge'] > 0.50) | (verticals_df['OfferEdge'] > 0.50)]\n",
    "    verticles_set3.to_csv(f\"verticles_set_3_{date}.csv\")\n",
    "    straddle_df = result_df[result_df['sprdtype'].str.contains(\"Straddle\")].copy()\n",
    "    straddle_df.to_csv(f\"straddles_{date}.csv\")\n",
    "    strangle_df = result_df[result_df['sprdtype'].str.contains(\"Strangle\")].copy()\n",
    "    strangle_df.to_csv(f\"strangles_{date}.csv\")\n",
    "    end_time_7 = time.perf_counter()\n",
    "    print(f\"time7: {end_time_7 - start_time_7}\")\n",
    "\n",
    "    start_time_8 = time.perf_counter()\n",
    "    zip_filename = f\"e.watchlist_{date}.zip\"\n",
    "    straddle_csv = f\"straddles_{date}.csv\"\n",
    "    vertical_csv = f\"verticals_{date}.csv\"\n",
    "    verticles_set1 = f\"verticles_set_1_{date}.csv\"\n",
    "    verticles_set2 = f\"verticles_set_2_{date}.csv\"\n",
    "    verticles_set3 = f\"verticles_set_3_{date}.csv\"\n",
    "    strangles_csv = f\"strangles_{date}.csv\"\n",
    "\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        zip_file.write(straddle_csv, arcname=os.path.basename(straddle_csv))\n",
    "        zip_file.write(vertical_csv, arcname=os.path.basename(vertical_csv))\n",
    "        zip_file.write(verticles_set1, arcname=os.path.basename(verticles_set1))\n",
    "        zip_file.write(verticles_set2, arcname=os.path.basename(verticles_set2))\n",
    "        zip_file.write(verticles_set3, arcname=os.path.basename(verticles_set3))\n",
    "        zip_file.write(strangles_csv, arcname=os.path.basename(strangles_csv))\n",
    "    \n",
    "    end_time_8 = time.perf_counter()\n",
    "    print(f\"time8: {end_time_8 - start_time_8}\")\n",
    "\n",
    "    #Send the zip archive via email\n",
    "    # send_email(['ewashington@scalptrade.com', 'sleland@scalptrade.com', 'aiacullo@scalptrade.com', 'jfeng@scalptrade.com', 'jthakkar@scalptrade.com ', 'jwood@scalptrade.com'], \n",
    "    #         'Open Rotation Watchlist', \n",
    "    #         attachment_path=zip_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.perf_counter()\n",
    "    run()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Total runtime: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
